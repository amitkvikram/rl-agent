{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an Agent to solve the openAI gym's mountain car problem.\n",
    "\n",
    "1. [Environment Details](https://github.com/openai/gym/wiki/MountainCar-v0)\n",
    "2. [Leaderboard Page](https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0)\n",
    "\n",
    "#### Mountain Car Problem: Get an under powered car to the top of a hill (top = 0.5 position)\n",
    "1. Action Space: motor = (left, neutral, right):int\n",
    "2. Observation Space: np.array([position, velocity])\n",
    "    - Veclocity = (-0.07, 0.07)\n",
    "    - Position = (-1.2, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b1b316e30044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import time\n",
    "from scipy.special import softmax\n",
    "# np.random.seed(0)\n",
    "\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self):\n",
    "        self.pos = None\n",
    "        self.vel = None\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.velocityLimit = np.array([env.observation_space.low[1], env.observation_space.high[1]])\n",
    "        self.positionLimit = np.array([env.observation_space.low[0], env.observation_space.high[0]])\n",
    "        self.velocityStep, self.positionStep = 0.005, 0.1\n",
    "        self.velocitySpace = np.arange(self.velocityLimit[0], self.velocityLimit[1] \n",
    "                                       + self.velocityStep, self.velocityStep)\n",
    "        self.positionSpace = np.arange(self.positionLimit[0], self.positionLimit[1] \n",
    "                                       + self.positionStep, self.positionStep)\n",
    "        self.m, self.n, self.n_action = len(self.velocitySpace), len(self.positionSpace), 3\n",
    "        self.Q = np.full(shape = (self.m, self.n, 3),\n",
    "                                       fill_value = 0.0, dtype = np.float32)\n",
    "        self.collectiveRecord = []\n",
    "        self.success = []\n",
    "        \n",
    "    def getActionValueIndex(self, state):\n",
    "        posOffset = state[0] - self.positionLimit[0]\n",
    "        velOffset = state[1] - self.velocityLimit[0]\n",
    "        posInd = posOffset // self.positionStep\n",
    "        velInd = velOffset // self.velocityStep\n",
    "        \n",
    "        return np.array([velInd, posInd], dtype= np.int)\n",
    "  \n",
    "    def getAction(self, state):\n",
    "        ind = self.getActionValueIndex(state, 0)\n",
    "        p = self.Policy[ind[0], ind[1], :]\n",
    "        action = np.random.choice([0, 1, 2], size = 1, p = p)\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: SARSA($\\lambda$) Agent\n",
    "\n",
    "### Method: \n",
    "1. Sarsa($\\lambda$) with $\\lambda$ = 0.8.\n",
    "2. States are represeted by discretiziing the position and velocity. \n",
    "2. epsilon-greedy policy with starting epsilon = 0.8 and decay rate = 0.995. \n",
    "3. starting learning-rate(alpha) = 0.8 and decay rate = 0.999.\n",
    "4. No reward shaping.\n",
    "\n",
    "#### Goal: \n",
    "- Solving the problem means average reward of -110 or more for 100 consecutive iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runSarsa(eps, alpha, LAMBDA, epsDecay, alphaDecay, numEpisodes, agent, env):\n",
    "    for i_eps in tqdm(range(1, numEpisodes + 1)):\n",
    "        state = env.reset()\n",
    "        agent.e = np.zeros(shape = (agent.m, agent.n, 3))\n",
    "        gamma = 1.0\n",
    "        ind = agent.getActionValueIndex(state)\n",
    "        if np.random.random() < 1 - eps:\n",
    "            action = np.argmax(agent.Q[ind[0], ind[1], :]) \n",
    "        else:\n",
    "            action = np.random.randint(0, 3)\n",
    "\n",
    "        for t in range(201):\n",
    "            ind = agent.getActionValueIndex(state)\n",
    "            nextState, reward, done, info = env.step(action)\n",
    "            nextInd = agent.getActionValueIndex(nextState)\n",
    "\n",
    "            if np.random.random() < 1 - eps:\n",
    "                nextAction = np.argmax(agent.Q[nextInd[0], nextInd[1], :]) \n",
    "            else: \n",
    "                nextAction = np.random.randint(0, 3)\n",
    "\n",
    "            delta = reward + gamma * agent.Q[nextInd[0],nextInd[1],nextAction] - agent.Q[ind[0],ind[1],action]\n",
    "            agent.e[ind[0],ind[1],action] += 1\n",
    "\n",
    "            agent.Q = np.add(agent.Q, np.multiply(alpha * delta, agent.e))\n",
    "            agent.e = np.multiply(gamma * LAMBDA, agent.e)\n",
    "\n",
    "            if done: \n",
    "                if t < 199:\n",
    "                    agent.success.append((i_eps, t))\n",
    "                agent.collectiveRecord.append(-t)\n",
    "                eps = max(0.0, eps * epsDecay)\n",
    "                alpha = max(0.0, alpha * alphaDecay)\n",
    "                break\n",
    "            state = nextState\n",
    "            action = nextAction\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(0)\n",
    "agent = Agent(env)\n",
    "print(\"Q Shape = \",agent.Q.shape)\n",
    "sleep(0.5)\n",
    "numEpisodes = 20000\n",
    "runSarsa(0.8, 0.2, 0.8, 0.998, 0.999, numEpisodes, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Print **reward** over 100 previous episodes for all the episodes during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(agent.collectiveRecord[:],'.')\n",
    "plt.yticks(range(-110, -200, -10))\n",
    "plt.ylabel(\"reward\")\n",
    "plt.xlabel(\"episode_number\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print **average reward** over 100 previous episodes for all the episodes during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgReward = []\n",
    "for i in range(100, numEpisodes):\n",
    "    avgReward.append(np.mean(agent.collectiveRecord[i - 100:i]))\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(avgReward, '.')\n",
    "plt.yticks(range(-110, -200, -10))\n",
    "# plt.xticks(changed_eps)\n",
    "plt.ylabel(\"Avg reward for last 100 episodes\")\n",
    "plt.xlabel(\"episode_number\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runSarsa(0.0, 0.0, 1.0, 1.0, 1.0, 100, agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Average Reward on 100 training episodes = {}\".format(np.mean(agent.collectiveRecord[-100:])))\n",
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "plt.plot(agent.collectiveRecord[-100:], '-')\n",
    "plt.yticks(range(-110, -200, -10))\n",
    "plt.title(\"Test Results\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"episode_number\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA($\\lambda$) with replacing traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSarsaWithRT(eps, alpha, LAMBDA, epsDecay, alphaDecay, numEpisodes, agent, env):\n",
    "    for i_eps in tqdm(range(1, numEpisodes + 1)):\n",
    "        state = env.reset()\n",
    "        agent.e = np.zeros(shape = (agent.m, agent.n, 3))\n",
    "        gamma = 1.0\n",
    "        ind = agent.getActionValueIndex(state)\n",
    "        if np.random.random() < 1 - eps:\n",
    "            action = np.argmax(agent.Q[ind[0], ind[1], :]) \n",
    "        else:\n",
    "            action = np.random.randint(0, 3)\n",
    "\n",
    "        for t in range(201):\n",
    "            ind = agent.getActionValueIndex(state)\n",
    "            nextState, reward, done, info = env.step(action)\n",
    "            nextInd = agent.getActionValueIndex(nextState)\n",
    "\n",
    "            if np.random.random() < 1 - eps:\n",
    "                nextAction = np.argmax(agent.Q[nextInd[0], nextInd[1], :]) \n",
    "            else: \n",
    "                nextAction = np.random.randint(0, 3)\n",
    "\n",
    "            delta = reward + gamma * agent.Q[nextInd[0],nextInd[1],nextAction] - agent.Q[ind[0],ind[1],action]\n",
    "#             agent.e[ind[0], ind[1], :] = 0\n",
    "            agent.e[ind[0], ind[1], action] = 1\n",
    "\n",
    "            agent.Q = np.add(agent.Q, np.multiply(alpha * delta, agent.e))\n",
    "            agent.e = np.multiply(gamma * LAMBDA, agent.e)\n",
    "\n",
    "            if done: \n",
    "                if t < 199:\n",
    "                    agent.success.append((i_eps, t))\n",
    "                agent.collectiveRecord.append(-t)\n",
    "                eps = max(0.0, eps * epsDecay)\n",
    "                alpha = max(0.0, alpha * alphaDecay)\n",
    "                break\n",
    "            state = nextState\n",
    "            action = nextAction\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "agent = Agent(env)\n",
    "print(\"Q Shape = \",agent.Q.shape)\n",
    "sleep(0.5)\n",
    "numEpisodes = 20000\n",
    "runSarsaWithRT(0.8, 0.2, 0.8, 0.995, 0.999, numEpisodes, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print **average reward** over 100 previous episodes for all the episodes during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgReward = []\n",
    "for i in range(100, numEpisodes):\n",
    "    avgReward.append(np.mean(agent.collectiveRecord[i - 100:i]))\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(avgReward, '.')\n",
    "plt.yticks(range(-110, -200, -10))\n",
    "# plt.xticks(changed_eps)\n",
    "plt.ylabel(\"Avg reward for last 100 episodes\")\n",
    "plt.xlabel(\"episode_number\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runSarsaWithRT(0.0, 0.0, 1.0, 1.0, 1.0, 100, agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Reward on 100 training episodes = {}\".format(np.mean(agent.collectiveRecord[-100:])))\n",
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "plt.plot(agent.collectiveRecord[-100:], '-')\n",
    "plt.yticks(range(-110, -200, -10))\n",
    "plt.title(\"Test Results\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"episode_number\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
